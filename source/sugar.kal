grammar  = require './grammar'
KEYWORDS = grammar.KEYWORDS

NOPAREN_WORDS = ['is','otherwise','except','else','doesnt','exist','exists','isnt','inherits','from','and','or','xor','in','when','instanceof','of','nor','if','unless','except']

function translate_sugar (tokens, show_tokens, tokenizer)
  out_tokens = coffee_style_functions noparen_function_calls multiline_statements clean code_in_strings tokens,  tokenizer
  if show_tokens
    debug = []
    for t in out_tokens
      if t.value is '\n'
        debug.push t.type
      else
        debug.push t.value or t.type
    console.log debug.join ' '
  return out_tokens
exports.translate_sugar = translate_sugar

function clean (tokens)
  # close out with a newline in case the user did not, remove whitespace
  out_tokens = []
  for token in tokens
    out_tokens.push(token) when token.type isnt 'WHITESPACE'
  return out_tokens

function multiline_statements (tokens)
  # allow multi-line statements with line breaks after commas, colons, and equal signs
  out_tokens = []
  last_token = null
  continue_line = no
  reduce_dedent = 0
  
  for token in tokens
    skip_token = no
    if last_token?.value in [',','=',':'] and token.type is 'NEWLINE'
      continue_line = yes
      skip_token = yes
    else if continue_line
      if token.type is 'INDENT'
        skip_token = yes
        reduce_dedent += 1
      else if token.type is 'NEWLINE'
        skip_token = yes
      else if token.type is 'DEDENT'
        if reduce_dedent > 0
          reduce_dedent -= 1
          skip_token = yes
          if reduce_dedent is 0
            out_tokens.push {text:'\n', line:token.line, value:'',type:'NEWLINE'}
        else
          out_tokens.push last_token #add back in the newline
    out_tokens.push(token) unless skip_token
    last_token = token
  return out_tokens
  
function noparen_function_calls (tokens)
  # allow function calls without parentheses
  out_tokens = []
  close_paren_count = 0
  last_token = null
  triggers = []
  closures = []
  ignore_next_indent = no
  
  i = 0
  while i < tokens.length
    token = tokens[i]
    last_token_callable = (last_token?.type is 'IDENTIFIER' and not (last_token.value in KEYWORDS)) or last_token?.value is ']'
    this_token_not_operator = ((token.type in ['IDENTIFIER','NUMBER','STRING','REGEX'] or token.value is '{') and not (token.value in NOPAREN_WORDS))
    if last_token_callable and this_token_not_operator
      triggers.push 'NEWLINE'
      out_tokens.push {text:'(', line:token.line, value:'(', type:'LITERAL'}
      closures.push ')'
    else if token.type is 'NEWLINE' and triggers[triggers.length-1] is 'NEWLINE' and tokens[i+1]?.type is 'INDENT'
      triggers[triggers.length-1] = 'DEDENT'
      ignore_next_indent = yes
    else if token.type is 'INDENT'
      if ignore_next_indent
        ignore_next_indent = no
      else
        triggers.push 'DEDENT'
        closures.push ''
    
    if (token.type is 'NEWLINE' or token.value in ['if','unless','when','except']) and closures.length > 0 and triggers[triggers.length - 1] is 'NEWLINE'
      while closures.length > 0 and triggers[triggers.length - 1] is 'NEWLINE'
        triggers.pop()
        closure = closures.pop()
        out_tokens.push({text:closure, line:token.line, value:closure, type:'LITERAL'}) if closure isnt ''
      out_tokens.push token
    else if token.type is 'DEDENT' and closures.length > 0 and triggers[triggers.length - 1] is 'DEDENT'
      out_tokens.push token
      triggers.pop()
      closure = closures.pop()
      out_tokens.push({text:closure, line:token.line, value:closure, type:'LITERAL'}) if closure isnt ''
    else if closures.length is 0 or token.type isnt triggers[triggers.length - 1]
      out_tokens.push token
    last_token = token
    i += 1
  return out_tokens

function coffee_style_functions (tokens)
  #allow function definitions with the -> operator
  out_tokens = []
  last_token = null
  
  i = 0
  while i < tokens.length
    token = tokens[i]
    if last_token?.value is '-' and token?.value is '>'
      out_tokens.pop() # remove the dash
      new_tokens = []
      t = out_tokens.pop()
      if t?.value is ')'
        while t?.value isnt '('
          new_tokens.unshift t
          t = out_tokens.pop()
        new_tokens.unshift t
      else
        out_tokens.push t
        new_tokens.push {text:'(', line:token.line, value:'(', type:'LITERAL'}
        new_tokens.push {text:')', line:token.line, value:')', type:'LITERAL'}
      f_token = {text:'function', line:token.line, value:'function', type:'IDENTIFIER'}
      new_tokens.unshift f_token
      out_tokens = out_tokens.concat new_tokens
    else
      # push the current token unchanged
      out_tokens.push token
    last_token = token
    i += 1
  return out_tokens
  
function code_in_strings (tokens, tokenizer)
  #allow double-quoted strings with embedded code, like: "x is #{x}"
  return tokens when tokenizer doesnt exist
  
  out_tokens = []
  for token in tokens
    if token.type is 'STRING' and token.value[0] is '"'
      rv = token.value
      r = /#{.*?}/g
      m = r.exec rv
      add_parens = yes if m otherwise no
      out_tokens.push({text:'(', line:token.line, value:'(', type:'LITERAL'}) when add_parens
      while m
        new_token_text = rv.slice(0,m.index) + '"'
        out_tokens.push {text:new_token_text, line:token.line, value:new_token_text, type:'STRING'}
        out_tokens.push {text:'+', line:token.line, value:'+', type:'LITERAL'}
        new_tokens = tokenizer(rv.slice(m.index+2,m.index+m[0].length-1))[0]
        out_tokens.push({text:'(', line:token.line, value:'(', type:'LITERAL'}) when new_tokens.length isnt 1
        out_tokens = out_tokens.concat new_tokens
        out_tokens.push({text:')', line:token.line, value:')', type:'LITERAL'}) when new_tokens.length isnt 1
        rv = '"' + rv.slice(m.index+m[0].length)
        if rv is '""' #avoid adding a (+ "") to strings that end with #{expr}"
          rv = ''
        else
          out_tokens.push {text:'+', line:token.line, value:'+', type:'LITERAL'}
        r = /#{.*?}/g
        m = r.exec rv
      out_tokens.push({text:rv, line:token.line, value:rv, type:'STRING'}) when rv isnt ''
      out_tokens.push({text:')', line:token.line, value:')', type:'LITERAL'}) when add_parens
    else
      out_tokens.push token
  return out_tokens
